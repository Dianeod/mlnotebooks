{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Demonstrations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we demonstrate how to use the Kx kdb+/q Automated Machine Learning library. The example below uses samples from the Telco Customer Churn dataset and IMBD movie review dataset.\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "To run the below notebook, ensure that dependencies specified in <b>requirements.txt</b> have been correctly installed.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Machine Learning Toolkit ([ML-Toolkit](https://github.com/KxSystems/ml)) contains general use utilities, an implementation of the FRESH (Feature Extraction based on Scalable Hypothesis tests) algorithm, cross validation functions, clustering libraries and time series functionality. The primary purpose of these libraries are to provide kdb+/q users with access to commonly-used ML functions for preprocessing data, extracting features and scoring results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated Machine Learning in kdb+/q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kdb+/q [Automated Machine Learning](https://github.com/kxsystems/automl/) library is built largely on the tools available within the kdb+/q ML-Toolkit. The purpose of this library is to provide users with the ability to automate the process of applying machine learning techniques to real-world problems. In the absence of expert machine learning engineers this framework handles the following processes within a traditional workflow:\n",
    "\n",
    "- Data preprocessing\n",
    "- Feature engineering and feature selection\n",
    "- Model selection\n",
    "- Hyperparameter tuning\n",
    "- Report generation and model persistence\n",
    "\n",
    "Each of these steps is outlined in depth within the documentation for this platform [here](https://code.kx.com/q/ml/automl). This allows users to understand the processes by which decisions are being made and the transformations which their data undergo during the production of the output models.\n",
    "\n",
    "At present the supported machine learning problem types for classification and regression tasks and based on:\n",
    "\n",
    "- One-to-one feature to target non time-series\n",
    "- FRESH based feature extraction and model production\n",
    "- NLP-based feature creation and word2vec transformation\n",
    "\n",
    "The problems which can be solved by this framework will be expanded over time as will the available functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-processing\n",
    "\n",
    "This library supports multi-processed grid-search/cross-validation procedures and FRESH feature creation provided a user set `-s -8` in the JUPYTERQ_SERVERARGS entry to the appropriate JSON file, instructions to facilitate this can be found [here](https://code.kx.com/q/ml/jupyterq/notebooks/#server-command-line-arguments). In this demo, we use 8 worker processes and open a centralised port as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\\p 5124"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Kx Automated Machine Learning library is then loaded in as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "// load in automl\n",
    "\\l automl/automl.q\n",
    ".automl.loadfile`:init.q\n",
    "\n",
    "// load utils\n",
    "\\l ../utils/util.q\n",
    "\\l ../utils/graphics.q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "Below, the notebook has been split into 3 distinct sections:\n",
    "1. [Default Configurations](#Default-Configurations)\n",
    "2. [Default NLP Configurations](#Default-NLP-Configurations)\n",
    "3. [Custom Configurations (Advanced)](#Custom-Configurations-(Advanced))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Telco Customer Churn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Telco Customer Churn dataset](https://www.kaggle.com/blastchar/telco-customer-churn/data) contains the following information.\n",
    "* Data on 7,043 customers of a telecom provider provided by IBM\n",
    "* Customer feature information including\n",
    "    * What form of internet the user has (DSL/Fiber Optic)?\n",
    "    * What are the users monthly payments?\n",
    "    * How long has the customer been in their contract?\n",
    "    * What services does the customer use? i.e. phone, internet, online backup, streaming etc.\n",
    "* A target variable 'Churn' indicating if a user has cancelled their contract in the last month\n",
    "\n",
    "\n",
    "In each case of the examples below, we aim to create a model which can accurately predict customer churn based on 20 features relating to each customer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\\S 42\n",
    "\n",
    "// load data and separate into features and target\n",
    "telcoData:5000?(\"SSBSSISSSSSSSSSSSSFFS\";(),\",\")0:`:../data/telco.csv\n",
    "telcoTarg:(`No`Yes!0 1)telcoData`Churn\n",
    "telcoFeat:delete Churn from telcoData\n",
    "\n",
    "// inspect data\n",
    "-1\"Shape of feature data is: \",(\" x \"sv string .ml.shape telcoFeat),\"\\n\";\n",
    "show 5#telcoFeat\n",
    "-1\"\\nDistribution of target values:\\n\";\n",
    "show update pcnt:.util.round[;.01]100*num%sum num from select num:count i by target from([]target:telcoTarg);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to test both the model generation and prediction steps of the workflow we split the dataset into a training and testing set where\n",
    "\n",
    "| Dataset form | Purpose                                                            | Percentage (%)|\n",
    "|--------------|:-------------------------------------------------------------------|---------------|\n",
    "| Training     | Generate model for deployment using `.automl.fit`                  | 90            |\n",
    "| Testing      | Independent dataset to test application of `predict` functionality | 10            |\n",
    "\n",
    "__*Note:*__ \n",
    "\n",
    "    We have set a random seed so that results can be replicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\\S 42\n",
    "show telcoInputs:.ml.traintestsplit[telcoFeat;telcoTarg;.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The automated machine learning pipeline will use the training features (`xtrain`) and targets (`ytrain`) from `telcoInputs` above as input to `automl.fit`. \n",
    "\n",
    "Appropriate preprocessing steps including feature creation and selection will be applied to the data before being passed to a variety of machine learning models, choosing the best performing model. \n",
    "\n",
    "In this case, we select ``` `normal``` feature extraction as we have a 1-to-1 mapping between features and targets. We also use ``` `class``` for the problem type as we are dealing with a binary classification problem.\n",
    "\n",
    "**Inportant:** \n",
    "\n",
    "    For the purposes of this demonstration we will pass in a dictionary in place of the default parameter (::). In order to ensure replication for users of this notebook the random seed parameter `seed is set in this example with the remaining parameters defaulted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "telcoFeats  :telcoInputs`xtrain       / features\n",
    "telcoTarget :telcoInputs`ytrain       / targets\n",
    "featureType1:`normal                  / normal feature extraction\n",
    "problemType1:`class                   / classification problem\n",
    "paramDict1  :enlist[`seed]!enlist 350 / default configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the default configuration, information generated during the fitting of the model will we saved to the outputs folder. This includes metadata information, graphs, reports and the fit model.\n",
    "\n",
    "In addition to saving outputs, the function returns a dictionary with two keys:\n",
    "\n",
    " Return key  | Description\n",
    "-------------|:-------------\n",
    " `modelInfo` | Metadata information generated from the pipeline such as preprocessing steps taken, significant features chosen and any other information needed to replicate the results.\n",
    " `predict`   | A function containing all relevant information and procedures required to generate new predictions using the fit model\n",
    "\n",
    "We can now run `.automl.fit` using the default setting with out training set from the Telco Customer Churn dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start:.z.t\n",
    "model1:.automl.fit[telcoFeats;telcoTarget;featureType1;problemType1;paramDict1]\n",
    "-1\"\\n.automl.fit took \",string .z.t-start;\n",
    "-1\"\\nReturn of .automl.fit:\\n\",.Q.s[model1];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature impact\n",
    "\n",
    "<img src=\"images/run1impact.png\" />\n",
    "\n",
    "We see that in the above example, 40 features were passed to the model following the application of feature extraction and significance testing. \n",
    "\n",
    "Looking at the feature impact plot above, we can see that `Contract_One year` had the highest feature impact in the dataset when applied to the best model, indicating this was the most important feature when generating predictions.\n",
    "\n",
    "#### Confusion matrix\n",
    "\n",
    "<img src=\"images/run1conf.png\" />\n",
    "\n",
    "A confusion matrix is also produced within the pipeline for classification problems. We see that the final `RandomForestClassifier` model correctly classified 782 data points. \n",
    "\n",
    "All other outputs from this run have been stored in a directory of format `/outputs/dateTimeModels/date/run_time/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply the workflow associated with our specified run to new data using the `predict` attribute returned\n",
    "\n",
    "The function will return the target predictions for new data based on the previously fitted model and workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-1\"Model applied to dataset:\\n\";\n",
    ".util.printDateTimeId model1.modelInfo;\n",
    "-1\"\\nPredictions: \";\n",
    "show pred1:model1.predict[telcoInputs`xtest]\n",
    "-1\"\\nTargets:\";\n",
    "show telcoInputs`ytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how the model performed using `.ml.accuracy`, used in the original default pipeline, and by producing a confusion matrix using `displayCM`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-1\"Accuracy on model run using hold-out data: \",string accuracy1:.ml.accuracy[telcoInputs`ytest;pred1];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".util.displayCM[value .ml.confmat[telcoInputs`ytest;pred1];`0`1;\"Test Set Confusion Matrix\";()];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default NLP Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `.automl.fit` can also be applied to text data using its default configuration.\n",
    "\n",
    "As with the example above, data must be presented with a 1-to-1 mapping between features and targets.\n",
    "\n",
    "NLP (Natural Language Processing) specific feature extraction is then applied to any character columns within the data using NLP methods contained within the Kx [NLP Library](https://github.com/KxSystems/nlp), while normal feature extraction is applied to any remaining features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMBD Dataset\n",
    "\n",
    "The [IMBD](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews) dataset contains reviews of over 50,000 movies for NLP or text analysis. The dataset consists of 2 columns, containing text reviews and the target indicating if they were positively or negatively reviewed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load NLP data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we load in 1500 rows of the IMDB dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// load data\n",
    "imdbData:1000#(\"SI\";enlist \",\")0:`:../data/IMBD.csv\n",
    "\n",
    "// convert text data to string\n",
    "imdbData:update string each comment from imdbData\n",
    "\n",
    "// separate into feature and target data\n",
    "imdbFeat:select comment from imdbData\n",
    "imdbTarg:imdbData`tgt\n",
    "\n",
    "// inspect data\n",
    "-1\"Shape of feature data is: \",(\" x \"sv string .ml.shape imdbFeat),\"\\n\";\n",
    "show 5#imdbFeat\n",
    "-1\"\\nDistribution of target values:\\n\";\n",
    "show update pcnt:.util.round[;.01]100*num%sum num from select num:count i by target from([]target:imdbTarg);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now split the data into training and testing sets to be used with `.automl.fit` and as an independent testing set for application of the `predict` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show imdbInputs:.ml.traintestsplit[imdbFeat;imdbTarg;.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below example demonstrates a binary classification problem. Notice that this time `nlp` is being passed as the feature extraction type.\n",
    "\n",
    "A slight modification will be made to the default parameters as this model will be saved under the name `nlpModelNotebook` and the overWriteFiles parameter will also be set to `1b` to allow users to run this notebook multiple times, overwriting the saved model each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMBDfeats   :imdbInputs`xtrain                                              / features\n",
    "IMBDtarget  :imdbInputs`ytrain                                              / targets\n",
    "featureType2:`nlp                                                           / NLP feature extraction\n",
    "problemType2:`class                                                         / classification problem\n",
    "paramDict2  :`savedModelName`overWriteFiles`seed!(`nlpModelNotebook;1b;100) / define name of model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run `automl.fit` utilizing the NLP functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "<b>Warning</b>: Please allow ample time for the NLP configuration to run as feature extraction and hyperparameter search can take upwards of 5 minutes to complete.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start:.z.t\n",
    ".automl.fit[IMBDfeats;IMBDtarget;featureType2;problemType2;paramDict2];\n",
    "-1\"\\n.automl.fit took \",string .z.t-start;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature impact\n",
    "\n",
    "<img src=\"images/run2impact.png\" />\n",
    "\n",
    "From the above example, we can see that even though one feature was passed to the model, multiple features were created using the `nlp` feature creation methods. If there was any additional non textual data present, the `normal` feature creation procedures would of been applied to them. \n",
    "\n",
    "Looking at the feature impact above, we can see that the features created by the `word2vec` module (`colx`) were deemed to be the most important along with various features created from the NLP spacy library\n",
    "\n",
    "#### Confusion matrix\n",
    "\n",
    "<img src=\"images/run2conf.png\" />\n",
    "\n",
    "A confusion matrix is also produced within the pipeline for classification problems. We see that the final `RandomForestClassifier` model correctly classified 129 out of 180 data points. \n",
    "\n",
    "All other outputs from this run have been stored in a directory of format `/outputs/namedModels/modelName/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To retrieve a model, `.automl.getModel` can be used to retrieve the metadata and associated prediction function to be used on new data from disk, either, by passing the name or the date/time of the desired model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show model2:.automl.getModel[enlist[`savedModelName]!enlist \"nlpModelNotebook\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model created within `automl.fit`  is applied to the unseen test data to evaluate the models performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-1\"Model applied to dataset:\\n\";\n",
    ".util.printSavedModelId model2.modelInfo;\n",
    "-1\"\\nPredictions: \";\n",
    "show pred2:model2.predict[imdbInputs`xtest]\n",
    "-1\"\\nTargets:\";\n",
    "show imdbInputs`ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-1\"Accuracy on model run using hold-out data: \",string accuracy2:.ml.accuracy[imdbInputs`ytest;pred2];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".util.displayCM[value .ml.confmat[imdbInputs`ytest;pred2];`0`1;\"Test Set Confusion Matrix\";()];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Configurations (Advanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section of the notebook, we showcased how to apply default parameters within the pipeline. In this section we will focus on how the final parameter of `.automl.fit` can be modified to apply changes to the default behaviour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we apply `.automl.featureCreation.normal.truncSingleDecomp` to the data, this is a truncated singular value decomposition outlined [here](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html) and applied to all combinations of columns of type float.\n",
    "\n",
    "A random seed of `200` will also be set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramKeys:`seed`functions                                         // parameter names to amend\n",
    "paramVals:(200;.automl.featureCreation.normal.truncSingleDecomp)  // amended values\n",
    "show paramDict3:paramKeys!paramVals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start:.z.t\n",
    "model3:.automl.fit[telcoInputs`xtrain;telcoInputs`ytrain;`normal;`class;paramDict3]\n",
    "-1\"\\n.automl.fit took \",string .z.t-start;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature impact\n",
    "<img src=\"images/run3impact.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see by looking at the feature impact that a number of the most impactful features are now derived from those generated when `.automl.featureCreation.normal.truncSingleDecomp` was applied during feature extraction, this gives some insight into the potential benefit of this form of feature extraction. \n",
    "\n",
    "While benefiting from increases in accuracy the addition of larger numbers of features can have the effect of slowing training time and scoring time which have have an impact in time critical use-cases.\n",
    "\n",
    "We can now predict on the hold-out dataset in order to compare accuracy results to the default case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-1\"Model applied to dataset:\\n\";\n",
    ".util.printDateTimeId model3.modelInfo;\n",
    "-1\"\\nPredictions: \";\n",
    "show pred3:model3.predict[telcoInputs`xtest]\n",
    "-1\"\\nTargets:\";\n",
    "show telcoInputs`ytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that by adding feature extraction in the normal case, we have improved the accuracy slightly increases. This is highlighted in the confusion matrix below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".util.printDateTimeId model1.modelInfo;\n",
    "-1\"Accuracy on default model run using held-out data: \",string[accuracy1],\"\\n\";\n",
    ".util.printDateTimeId model3.modelInfo;\n",
    "-1\"Accuracy on custom model run using held-out data : \",string .ml.accuracy[telcoInputs`ytest;pred3];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".util.displayCM[value .ml.confmat[telcoInputs`ytest;pred3];`0`1;\"Test Set Confusion Matrix\";()];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we again use the Telco dataset and highlight how to change the save options contained under `saveOption` within the parameter dictionary.\n",
    "\n",
    "In the default case, not modified in the examples above, the system will save all outputs to disk (reports, images, config file and models). Below we will set the `saveOption` to be `0`, which means that the results will be displayed to console but nothing is persisted to disk.\n",
    "\n",
    "Other alterations made to the `paramDict` in the below model were\n",
    "1. Added a **random seed**: Here we have altered the ``` `seed``` parameter to be `175`.\n",
    "2. Changed the size of the **holdout** sets to be 30% of the data at each stage.\n",
    "3. Changed the **hyperparameter search** type from the default of grid search to random search and set the number of repetitions to `2`. Note that Sobol-random search is also available.\n",
    "\n",
    "A smaller subset of 1000 datapoints will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\\S 42\n",
    "features:1000?telcoFeat\n",
    "target  :1000?telcoTarg\n",
    "\n",
    "paramKeys:`saveOption`seed`holdoutSize`hyperparameterSearchType`randomSearchArgument // parameter names to amend\n",
    "paramVals:(0;175;.3;`random;2)                                                       // amended values\n",
    "show paramDict:paramKeys!paramVals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    ".automl.fit[features;target;`normal;`class;paramDict];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see compared to the previous default behaviour nothing has been saved down during a single run of the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the IMDB dataset is used with the following changes made to the input dictionary `paramDict`:\n",
    "\n",
    "1. **Word2vec transformation** changed from default `continuous bag of words` method to `skip-gram`.\n",
    "2. **Significant feature function** changed to use the percentile based procedure.\n",
    "3. **Random seed** set as `275`.\n",
    "\n",
    "In this example, printing to screen will also be suppressed and redirected to a log file called `LogFile`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".automl.updatePrinting[]   // Disable printing to screen \n",
    ".automl.updateLogging[]    // Redirect printing to log file\n",
    "\n",
    "\n",
    "// new significant feature function \n",
    ".automl.newSigFeat:{[x;y]\n",
    "  .ml.fresh.significantfeatures[x;y;.ml.fresh.percentile 0.10]\n",
    "  }\n",
    "\n",
    "// new parameter dictionary\n",
    "paramKeys:`significantFeatures`w2v`seed`loggingFile  // parameter names to amend\n",
    "paramVals:(`.automl.newSigFeat;1;275;\"logFile\")      // amended values\n",
    "show paramDict4:paramKeys!paramVals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "// run automl with new parameters\n",
    "model4:.automl.fit[imdbInputs`xtrain;imdbInputs`ytrain;`nlp;`class;paramDict4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the features deemed important compared with the initial run have varied. In this iteration, very few of the features created from the NLP spacy library were deemed to be significant when predicting the target value\n",
    "\n",
    "<img src=\"images/run4impact.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".util.printDateTimeId model4.modelInfo;\n",
    "-1\"\\nPredictions: \";\n",
    "show pred4:model4.predict[imdbInputs`xtest]\n",
    "-1\"\\nTargets:\";\n",
    "show imdbInputs`ytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we can see how changing the `w2v` implementation decreased the accuracy of the model compared with the initial run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ".util.printDateTimeId model2.modelInfo;\n",
    "-1\"Accuracy on default model run using held-out data: \",string[accuracy2],\"\\n\";\n",
    ".util.printDateTimeId model4.modelInfo;\n",
    "-1\"Accuracy on custom model run using held-out data : \",string .ml.accuracy[imdbInputs`ytest;pred4];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Q (kdb+)",
   "language": "q",
   "name": "qpk"
  },
  "language_info": {
   "file_extension": ".q",
   "mimetype": "text/x-q",
   "name": "q",
   "version": "4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
