{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Demonstrations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we showcase how to use kdb+/q paired with embedPy to carry out machine learning tasks. Below we show how to train and test a decision tree classifier to identify benign/malignant samples from the Wisconsin Breast Cancer dataset.\n",
    "\n",
    "<div class=\"alert alert-danger\">\n",
    "To run the below notebook, ensure that dependencies specified in <b>requirements.txt</b> have been correctly installed.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EmbedPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EmbedPy allows the kdb+/q interpreter to manipulate Python objects, call Python functions and load Python libraries. Python and kdb+/q developers can fuse both technologies together, allowing for a seamless application of kdb+/q’s high-speed analytics and Python’s rich ecosystem of libraries such as scikit-learn, Tensorflow, PyTorch and Theano."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Using Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees are a simple yet effective algorithm used for supervised classification and regression problems.\n",
    "\n",
    "A decision tree is made up of a collection of simple hierarchical decision rules, classifying datapoints into categories by splitting them based on feature values. The task of fitting a decision tree to data therefore involves finding the sequence of feature splits and the optimal split values.\n",
    "\n",
    "Decision trees can:\n",
    "- Manage a mixture of discrete, continuous and categorical inputs.\n",
    "- Use data with no normalization/preprocessing (including missing data).\n",
    "- Produce a highly interpretable output, which can be easily explained and visualized.\n",
    "\n",
    "Further discussion of decision trees can be found in the Wikipedia article [Decision tree](https://en.wikipedia.org/wiki/Decision_tree) or [Sci-Kit Learn documentation](http://scikit-learn.org/stable/modules/tree.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breast Cancer Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Wisconsin Breast Cancer Dataset](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29) is a set of 569 samples of fine needle aspirate (FNA) of breast mass. Each sample contains features describing characteristics of the cell nuclei, along with a classification of the sample as either benign or malignant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "\n",
    "In the cell below, we load in the utilities library from the kdb+/q [ML-Toolkit](https://github.com/KxSystems/ml), along with graphics functions required for this notebook. We then use embedPy to import relevant python modules and load data from the Wisconsin Breast Cancer dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "/ load toolkit and graphics functions\n",
    "\\l ml/ml.q\n",
    ".ml.loadfile`:util/init.q\n",
    "\n",
    "\\l ../utils/graphics.q\n",
    "\\l ../utils/util.q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of feature data is: 569 x 30\n",
      "\n",
      "17.99 10.38 122.8 1001  0.1184  0.2776  0.3001 0.1471  0.2419 0.07871 1.095  ..\n",
      "20.57 17.77 132.9 1326  0.08474 0.07864 0.0869 0.07017 0.1812 0.05667 0.5435 ..\n",
      "19.69 21.25 130   1203  0.1096  0.1599  0.1974 0.1279  0.2069 0.05999 0.7456 ..\n",
      "11.42 20.38 77.58 386.1 0.1425  0.2839  0.2414 0.1052  0.2597 0.09744 0.4956 ..\n",
      "20.29 14.34 135.1 1297  0.1003  0.1328  0.198  0.1043  0.1809 0.05883 0.7572 ..\n",
      "\n",
      "Distribution of target values:\n",
      "\n",
      "target| num pcnt \n",
      "------| ---------\n",
      "0     | 212 37.26\n",
      "1     | 357 62.74\n"
     ]
    }
   ],
   "source": [
    "/ load in data\n",
    "data:.p.import[`sklearn.datasets;`:load_breast_cancer][]\n",
    "feat:data[`:data]`\n",
    "targ:data[`:target]`\n",
    "featnm:.p.list[<]data`:feature_names\n",
    "\n",
    "/ inspect data\n",
    "-1\"Shape of feature data is: \",(\" x \"sv string .ml.shape feat),\"\\n\";\n",
    "show 5#feat\n",
    "-1\"\\nDistribution of target values:\\n\";\n",
    "show update pcnt:.util.round[;.01]100*num%sum num from select num:count i by target from([]target:targ);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output above shows that classes are quite unbalanced:\n",
    "- `37%` are malignant (0)\n",
    "- `63%` are benign (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data\n",
    "Before we can train a model we need to split the original data into training and testing sets. Below we select 50% to be present in the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xtrain| 284\n",
      "ytrain| 284\n",
      "xtest | 285\n",
      "ytest | 285\n"
     ]
    }
   ],
   "source": [
    "\\S 123  / random seed\n",
    "show count each datadict:.ml.trainTestSplit[feat;targ;.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and train the model\n",
    "\n",
    "At this stage it is possible to fit the data to a decision tree classifier model, restricting the tree to a maximum depth of 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf:.p.import[`sklearn.tree]`:DecisionTreeClassifier\n",
    "clf:clf[`max_depth pykw 3]\n",
    "clf[`:fit][datadict`xtrain;datadict`ytrain];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the graph visualization software [Graphviz](https://www.graphviz.org/), we can look at the structure of the resulting decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/dianeodonoghue/miniconda3/lib/python3.7/site-packages/graphviz/backend.py\", line 123, in pipe\n",
      "    stdout=subprocess.PIPE, startupinfo=STARTUPINFO)\n",
      "  File \"/Users/dianeodonoghue/miniconda3/lib/python3.7/subprocess.py\", line 800, in __init__\n",
      "    restore_signals, start_new_session)\n",
      "  File \"/Users/dianeodonoghue/miniconda3/lib/python3.7/subprocess.py\", line 1551, in _execute_child\n",
      "    raise child_exception_type(errno_num, err_msg, err_filename)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'dot': 'dot'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/dianeodonoghue/miniconda3/lib/python3.7/site-packages/IPython/core/formatters.py\", line 224, in catch_format_error\n",
      "    r = method(self, *args, **kwargs)\n",
      "  File \"/Users/dianeodonoghue/miniconda3/lib/python3.7/site-packages/IPython/core/formatters.py\", line 345, in __call__\n",
      "    return method()\n",
      "  File \"/Users/dianeodonoghue/miniconda3/lib/python3.7/site-packages/graphviz/files.py\", line 83, in _repr_svg_\n",
      "    return self.pipe(format='svg').decode(self._encoding)\n",
      "  File \"/Users/dianeodonoghue/miniconda3/lib/python3.7/site-packages/graphviz/files.py\", line 98, in pipe\n",
      "    outs = backend.pipe(self._engine, format, data)\n",
      "  File \"/Users/dianeodonoghue/miniconda3/lib/python3.7/site-packages/graphviz/backend.py\", line 128, in pipe\n",
      "    'are on your systems\\' path' % args)\n",
      "RuntimeError: failed to execute ['dot', '-Tsvg'], make sure the Graphviz executables are on your systems' path\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<graphviz.files.Source at 0x12c4cce90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gv: .p.import`graphviz\n",
    "egv:.p.import[`sklearn.tree;`:export_graphviz]\n",
    "dotdata:egv[clf;`out_file pykw (::);`feature_names pykw featnm]\n",
    "graph:gv[`:Source]dotdata\n",
    ".util.display graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision tree classifier produces a highly interpretable model which can be visualized and understood even by those with a less technical knowledge.\n",
    "\n",
    "The algorithm finds the best tree by following a greedy strategy where it looks for the feature (mean concave points) and split value (0.052) that most effectively partitions the data.\n",
    "\n",
    "This divides the dataset of 284 samples into two subsets of 176 samples and 108 samples:\n",
    "- Of the 176 samples, 7 (4%) are malignant and 169 (96%) are benign.\n",
    "- Of the 108 samples, 95 (88%) are malignant and 13 (12%) are benign.\n",
    "\n",
    "The algorithm continues splitting the dataset at each node, by finding the feature and split value that most effectively partitions the benign from the malignant samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model\n",
    "\n",
    "The output of the decision tree is a class assignment.\n",
    "\n",
    "We take a previously unseen sample and pass it through the decision tree. Following the appropriate branch at each split (based on the feature values of the test point), we eventually end up at a leaf node, at the bottom of the tree. At this point, we assign the test point the class value of the majority of the training examples included in that leaf.\n",
    "\n",
    "We can therefore evaluate the performance of the decision tree on the held-out test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/ make predictions\n",
    "yprob:clf[`:predict_proba;<]datadict`xtest\n",
    "ypred:yprob?'max each yprob\n",
    "ytest:datadict`ytest\n",
    "\n",
    "/ calculate performance metrics\n",
    "dtloss:.ml.logLoss[ytest;yprob]\n",
    "dtacc:.ml.accuracy[ypred;ytest]\n",
    "\n",
    "-1\"Performance of the classifier\";\n",
    "-1\"log loss: \",string[dtloss],\"\\naccuracy: \",string dtacc;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision tree classifier achieves 95% accuracy on the test set, a strong performance from such a simple classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix\n",
    "\n",
    "\n",
    "With a Confusion Matrix, we can inspect the interaction between:\n",
    "- True positives\n",
    "- True negatives\n",
    "- False positives\n",
    "- False negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show cnfM:.ml.confMatrix[ypred;ytest]\n",
    ".util.displayCM[value cnfM;`benign`malignant;\"Test Set Confusion Matrix\";()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier has:\n",
    "\n",
    "- True Positive Rate: ```TPR = TP/(TP+FN) = 173/(173+2) = 99%```\n",
    "- False Positive Rate: ```FPR = FP/(FP+TN) = 12/(12+98) =  11%```\n",
    "\n",
    "**NB**: We are using *positive* here to denote the malignant case, which actually has the label `0`, rather than `1` in the Wisconsin dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve\n",
    "Rather than using a majority vote system, we could use a threshold other than 50% for assigning the points to classes at the leaf node.\n",
    "\n",
    "- By modifying the threshold in favour of a _malignant_ diagnosis, we would increase the true positive rate, but would also increase the false positive rate.  \n",
    "- By modifying the threshold in favour of a _benign_ diagnosis, we would decrease the false positive rate, but would also decrease the true positive rate.  \n",
    "\n",
    "The realtionship between the true positive rate (sensitivity) and the false positive rate, is captured in the Receiver Operating Characteristic (ROC) curve.\n",
    "\n",
    "Receiver Operating Characteristic (ROC) describes the ability to increase the recall/sensitivity of a model, without decreasing the precision of the model. The ROC value is calulated as the area under the associated ROC curve (plotting true positive rates against false positive rates).\n",
    "\n",
    "**NB**: `1 - FPR` is called the specificity of the model. We therefore have a trade-off between sensitivity and specificity.\n",
    "\n",
    "The area under the curve (AUC) is interpreted as the probability that a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one.\n",
    "\n",
    "The optimal classifier would have a false positive rate of 0 and a true positive rate of 1, giving a an AUC curve of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yprob:clf[`:predict_proba;feat]`\n",
    ".util.displayROC[targ;yprob[;1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " With an AUC of 0.96, our classifier is close to optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Q (kdb+)",
   "language": "q",
   "name": "qpk"
  },
  "language_info": {
   "file_extension": ".q",
   "mimetype": "text/x-q",
   "name": "q",
   "version": "4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
